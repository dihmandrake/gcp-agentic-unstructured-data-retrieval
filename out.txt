diff --git a/src/agents/adk_agent.py b/src/agents/adk_agent.py
index 3043249..264acd9 100644
--- a/src/agents/adk_agent.py
+++ b/src/agents/adk_agent.py
@@ -3,7 +3,29 @@ from google.adk.models.google_llm import Gemini
 from src.agents.tools import search_knowledge_base
 from google.genai import types
 
-system_prompt = """You are a helpful AI assistant.
+system_prompt = """
+# =================================================================================================
+# TODO: HACKATHON CHALLENGE (Pillar 1: Completeness)
+#
+# The current system prompt provides a basic persona. Your challenge is to enhance it to make the
+# agent more robust, reliable, and aligned with specific use cases (e.g., a medical assistant,
+# a customer service bot, a technical support agent).
+#
+# REQUIREMENTS:
+#   1. PERSONA: Give the agent a more specific and detailed persona. Consider:
+#      - What is its role (e.g., "You are a highly experienced medical assistant...")?
+#      - What are its core competencies or areas of expertise?
+#      - What is its tone and communication style (e.g., "...always respond with empathy and clarity.")?
+#   2. STRICT INSTRUCTIONS:
+#      - Add explicit rules or constraints to guide the agent's behavior.
+#      - How should it handle ambiguous queries? What if information is missing?
+#      - Should it ask clarifying questions? Should it refuse to answer certain types of questions?
+#      - Emphasize the importance of using the `search_knowledge_base` tool and citing sources.
+#
+# HINT: Think about edge cases and how a human expert in this persona would behave.
+# =================================================================================================
+
+You are a helpful AI assistant.
 Your knowledge comes exclusively from the documents provided to you via the "search_knowledge_base" tool.
 You must ALWAYS use the "search_knowledge_base" tool to find information before answering any question.
 Do not rely on any prior knowledge."""
diff --git a/src/ingestion/chunker.py b/src/ingestion/chunker.py
index 690d395..c25309d 100644
--- a/src/ingestion/chunker.py
+++ b/src/ingestion/chunker.py
@@ -5,7 +5,7 @@ logger = setup_logger(__name__)
 
 def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
     """
-    Splits text into context-aware segments using a simple sliding window approach.
+    Splits text into context-aware segments.
 
     Args:
         text (str): The input text to chunk.
@@ -15,6 +15,32 @@ def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[st
     Returns:
         List[str]: A list of text chunks.
     """
+    
+    # =================================================================================================
+    # TODO: HACKATHON CHALLENGE (Pillar 1: Completeness)
+    #
+    # The current chunking logic is a basic, fixed-size sliding window. This is inefficient and
+    # often splits sentences or paragraphs in awkward places, leading to poor context for the LLM.
+    #
+    # Your challenge is to replace this naive implementation with a more intelligent chunking strategy.
+    #
+    # REQUIREMENT: You must implement ONE of the following advanced chunking methods:
+    #
+    #   1. RECURSIVE CHUNKING:
+    #      - Split the text recursively by a list of separators (e.g., "\n\n", "\n", " ", "").
+    #      - This method tries to keep paragraphs, sentences, and words together as long as possible.
+    #      - HINT: Look at how libraries like LangChain implement `RecursiveCharacterTextSplitter`.
+    #
+    #   2. SEMANTIC CHUNKING:
+    #      - Use a sentence embedding model (like `text-embedding-004`) to measure the semantic
+    #        similarity between consecutive sentences.
+    #      - Split the text where the similarity score drops, indicating a change in topic.
+    #      - This is the most advanced method and will likely yield the best RAG performance.
+    #      - HINT: You'll need to calculate cosine similarity between sentence embeddings.
+    #
+    # =================================================================================================
+
+    # Naive, fixed-size chunking (TO BE REPLACED)
     if overlap >= chunk_size:
         logger.warning("Overlap is greater than or equal to chunk_size. Adjusting overlap to be chunk_size - 1.")
         overlap = chunk_size - 1
@@ -26,12 +52,6 @@ def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[st
         chunk = text[start:end]
         chunks.append(chunk)
         start += (chunk_size - overlap)
-        if start >= len(text) and end < len(text):
-            # Add the last remaining part if it's smaller than chunk_size but not yet added
-            remaining = text[end:]
-            if remaining:
-                chunks.append(remaining)
 
     logger.info(f"Chunked text into {len(chunks)} segments with chunk_size={chunk_size} and overlap={overlap}.")
-    # TODO: This is where 'Semantic Chunking' logic should be injected for more advanced use cases.
     return chunks
diff --git a/src/ingestion/parser.py b/src/ingestion/parser.py
index b922ddf..ef8ab91 100644
--- a/src/ingestion/parser.py
+++ b/src/ingestion/parser.py
@@ -19,8 +19,42 @@ def parse_pdf(file_path: str) -> str:
         for page in reader.pages:
             text += page.extract_text() + "\n"
         logger.info(f"Successfully parsed PDF: {file_path}")
-        # TODO: Implement OCR for scanned documents here if needed in the future.
+        # TODO: HACKATHON CHALLENGE (Optional, but good for completeness)
+        # If you want to handle scanned PDFs (images of text), you would integrate an OCR (Optical Character Recognition)
+        # library here, such as Google Cloud Vision AI or Tesseract. This is not a core requirement for the hackathon,
+        # but a valuable extension for real-world unstructured data.
         return text
     except Exception as e:
         logger.error(f"Error parsing PDF {file_path}: {e}")
         raise
+
+def parse_other_format(file_path: str) -> str:
+    """
+    Placeholder for parsing other document formats.
+
+    Args:
+        file_path (str): The absolute path to the file.
+
+    Returns:
+        str: The extracted text content.
+    """
+    # =================================================================================================
+    # TODO: HACKATHON CHALLENGE (Pillar 2: Extensibility)
+    #
+    # This function is a placeholder. Your challenge is to implement logic to parse at least one
+    # new file format (e.g., .txt, .csv, .docx, .html) beyond PDFs.
+    #
+    # REQUIREMENTS:
+    #   1. Choose a new file format to support (e.g., plain text, CSV, Word document, HTML).
+    #   2. Implement the necessary code to read the content of that file type and return it as a string.
+    #   3. You may need to install new libraries (e.g., `python-docx` for .docx, `pandas` for .csv).
+    #      Remember to add any new dependencies to `pyproject.toml` and install them with `poetry install`.
+    #   4. Ensure robust error handling for unsupported formats or corrupted files.
+    #
+    # HINT: For simple text files, you can just read the file content directly.
+    #       For CSV, you might read it into a pandas DataFrame and then convert it to a string representation.
+    #       For HTML, you could use BeautifulSoup to extract visible text.
+    #
+    # =================================================================================================
+    logger.warning(f"Parsing for {file_path} is not yet implemented. Returning empty string.")
+    return "" # Placeholder, replace with actual parsing logic
diff --git a/src/ingestion/pipeline.py b/src/ingestion/pipeline.py
index 99c9ddb..a08f3d1 100644
--- a/src/ingestion/pipeline.py
+++ b/src/ingestion/pipeline.py
@@ -6,6 +6,7 @@ from google.cloud import storage
 from src.shared.logger import setup_logger
 from src.search.vertex_client import VertexSearchClient
 from src.shared.sanitizer import sanitize_id
+from src.ingestion.parser import parse_pdf, parse_other_format # Import the new parser
 
 logger = setup_logger(__name__)
 
@@ -23,18 +24,33 @@ def run_ingestion(input_dir: str, output_dir: str):
         return
 
     os.makedirs(output_dir, exist_ok=True)
-    pdf_files = glob(os.path.join(input_dir, "*.pdf"))
+    
+    # =================================================================================================
+    # TODO: HACKATHON CHALLENGE (Pillar 2: Extensibility)
+    #
+    # The current pipeline only processes PDF files. Your challenge is to extend it to support
+    # the new file format you implemented in `src/ingestion/parser.py`.
+    #
+    # REQUIREMENTS:
+    #   1. Modify the `glob` pattern below to include your new file type (e.g., "*.pdf", "*.txt").
+    #   2. Implement logic within the loop to detect the file type and call the appropriate
+    #      parser function (`parse_pdf` or `parse_other_format`).
+    #   3. Ensure the `mimeType` in the `metadata_list` entry is correctly set for your new file type.
+    #
+    # HINT: You can use `file_name.lower().endswith(".your_extension")` to check the file type.
+    # =================================================================================================
+    all_files = glob(os.path.join(input_dir, "*.pdf")) # Extend this glob to include your new file type
 
-    if not pdf_files:
-        logger.warning(f"No PDF files found in input directory: {input_dir}")
+    if not all_files:
+        logger.warning(f"No files found in input directory: {input_dir}")
         return
 
     storage_client = storage.Client()
     bucket = storage_client.bucket(gcs_bucket_name)
     metadata_list = []
 
-    logger.info(f"--- Uploading {len(pdf_files)} PDF files to GCS ---")
-    for file_path in pdf_files:
+    logger.info(f"--- Uploading {len(all_files)} files to GCS ---")
+    for file_path in all_files:
         try:
             file_name = os.path.basename(file_path)
             gcs_raw_path = f"raw/{file_name}"
@@ -46,11 +62,19 @@ def run_ingestion(input_dir: str, output_dir: str):
 
             base_name = os.path.splitext(file_name)[0]
             doc_id = sanitize_id(base_name)
+            
+            # Determine mimeType based on file extension
+            mime_type = "application/pdf" # Default to PDF, update this based on your new file type logic
+            # if file_name.lower().endswith(".txt"):
+            #     mime_type = "text/plain"
+            # elif file_name.lower().endswith(".csv"):
+            #     mime_type = "text/csv"
+
             metadata_list.append({
                 "id": doc_id,
                 "structData": {"source_file": file_name},
                 "content": {
-                    "mimeType": "application/pdf",
+                    "mimeType": mime_type,
                     "uri": gcs_uri
                 }
             })
@@ -76,27 +100,36 @@ def run_ingestion(input_dir: str, output_dir: str):
         logger.error(f"Failed to trigger Vertex AI import: {e}")
 
     # Also generate a local processed_data.json for chunking visibility
-    _generate_local_processed_data(pdf_files, output_dir)
+    _generate_local_processed_data(all_files, output_dir)
 
-def _generate_local_processed_data(pdf_files: list[str], output_dir: str):
+def _generate_local_processed_data(files: list[str], output_dir: str):
     """
-    Parses PDFs locally and saves the output to a JSON file for inspection.
+    Parses files locally and saves the output to a JSON file for inspection.
     This is a simulation of the chunking that Vertex AI would perform.
     """
     logger.info("--- Generating local processed_data.json for chunking visibility ---")
     processed_data = []
 
-    for file_path in pdf_files:
+    for file_path in files:
         try:
             file_name = os.path.basename(file_path)
-            reader = pypdf.PdfReader(file_path)
-            for i, page in enumerate(reader.pages):
+            text_content = ""
+            if file_name.lower().endswith(".pdf"):
+                reader = pypdf.PdfReader(file_path)
+                for page in reader.pages:
+                    text_content += page.extract_text() + "\n"
+            else:
+                # TODO: HACKATHON CHALLENGE (Pillar 2: Extensibility)
+                # Call your new parser function here for other file types.
+                # Example: text_content = parse_other_format(file_path)
+                text_content = parse_other_format(file_path) # Placeholder
+
+            if text_content:
                 processed_data.append({
-                    "id": sanitize_id(f"{file_name}_page_{i+1}"),
+                    "id": sanitize_id(f"{file_name}"),
                     "structData": {
-                        "text_content": page.extract_text(),
+                        "text_content": text_content,
                         "source_file": file_name,
-                        "page_number": i + 1,
                     }
                 })
         except Exception as e:
@@ -108,4 +141,3 @@ def _generate_local_processed_data(pdf_files: list[str], output_dir: str):
             f.write(json.dumps(entry) + "\n")
     
     logger.info(f"Local processed data saved to: {output_file_path}")
-
diff --git a/src/search/vertex_client.py b/src/search/vertex_client.py
index b1298d9..a6d3f48 100644
--- a/src/search/vertex_client.py
+++ b/src/search/vertex_client.py
@@ -42,6 +42,29 @@ class VertexSearchClient:
         Executes a search query against the Vertex AI Search data store.
         """
         try:
+            # =================================================================================================
+            # TODO: HACKATHON CHALLENGE (Pillar 1: Completeness)
+            #
+            # The current search is a basic keyword search. Your challenge is to enhance it using
+            # Vertex AI Search's advanced capabilities.
+            #
+            # REQUIREMENT: You must implement ONE of the following search enhancements:
+            #
+            #   1. HYBRID SEARCH:
+            #      - Combine keyword-based search with vector-based (semantic) search.
+            #      - This typically involves setting `query_expansion_spec` and `spell_correction_spec`
+            #        in the `SearchRequest` to leverage Vertex AI's built-in capabilities.
+            #      - HINT: Explore `query_expansion_spec` and `spell_correction_spec` within
+            #        `discoveryengine.SearchRequest`.
+            #
+            #   2. METADATA FILTERING:
+            #      - Allow the search to be filtered based on document metadata (e.g., `source_file`, `page_number`).
+            #      - This requires adding a `filter` parameter to the `SearchRequest`.
+            #      - HINT: The `filter` parameter accepts a string with filter conditions, e.g.,
+            #        `"structData.source_file:exact_match('medical_record_John_Doe.pdf')"`.
+            #
+            # =================================================================================================
+
             content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(
                 snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(
                     return_snippet=True
